\documentclass{article}
\usepackage{graphicx} % Required for inserting images

\usepackage{auto-pst-pdf} % Enable PSTricks with pdflatex
% If using Overleaf, the -shell-escape flag is already enabled by default for auto-pst-pdf to work
\usepackage{pst-eucl} % Euclidean geometry

\usepackage{tikz} % ChatGPT likes this for drawing some things
\usetikzlibrary{calc,intersections}

\usepackage{mathtools}

\usepackage{hyperref}


\usepackage{caption} % for being able to put a title for a figure above it (in addition to the caption below it)


\usepackage{titlesec}
\usepackage{amsmath} % for equation*

% Setup sectioning commands
\titleformat{\section}
  {\normalfont\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}
  {\normalfont\large\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries}{\thesubsubsection}{1em}{}
\titleformat{\paragraph}
  {\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}

% Define subsubsubsection
\titleclass{\subsubsubsection}{straight}[\subsubsection]
\newcounter{subsubsubsection}[subsubsection]
\renewcommand{\thesubsubsubsection}{\thesubsubsection.\arabic{subsubsubsection}}
\titleformat{\subsubsubsection}
  {\normalfont\normalsize\bfseries}{\thesubsubsubsection\quad}{0em}{#1\newline}
\titlespacing*{\subsubsubsection}
  {0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}



% Adjust depth of section numbering and TOC inclusion
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}



\title{MAT 357/557 Numerical Analysis Project 3}
\author{Austin Zhu, Brillan Morgan, Bhargav Vundavalli}
\date{Spring 2024}

\begin{document}

\maketitle



\section{The Motivation}
\label{sec:the_motivation}
There are countless practical applications of collecting data over time and storing it for later use. An application may use time-series data: a set of points where each point consists of some value and its associated time.

\subsection{More Data Than Can Be Stored}
\label{subsec:practical_applications}
For some applications, time series data may be collected frequently, potentially many times per second. \textbf{(Give example.) } Time-series data may also be collected over the span of a long time, possibly over the course of months or years. \textbf{(Example). } Some time-series data may be both collected very frequently and for a very long time. \textbf{Example.} In these types of situations, it may be impractical to store the value and time of each data point with perfect accuracy: the data may exceed the available storage. This is especially relevant in embedded applications where there is little local storage available and no (or no reliable) network connection.

\subsection{Selectively Discard Unnecessary Accuracy}
\label{subsec:unnecessary_accuracy}
Further, depending on the application, perfect accuracy of times and/or values may simply be unnecessary, so the extra space taken to store perfectly accurate values would be a waste of valuable storage resources. \textbf{(Example)}

\section{The Idea: Best-Fit Piecewise Interpolating Polynomials}
\label{sec:the_idea}
As an alternative to storing exact time and value data for each point in a time-series data stream, we propose a highly adjustable, on-the-fly compression system using best-fit piecewise interpolating polynomials.

\subsection{Deciding What to Fit}
\textbf{AVOID STACKED HEADINGS: put some intro text here}
\subsubsection{Using Best-Fit With a Common Approach}
A common approach to storing time-series data points is to record time as the first coordinate (the "x") and to record the data value as the second coordinate (the "y"). Best-fit piecewise polynomials \textit{could} be used to encode the data given this approach. However, since time would be the independent variable, time data with perfect accuracy while reducing the accuracy of what is presumably the more precious half of the information: the data values being collected!

\subsubsection{Fitting Time and Value Data Separately}
\textbf{TODO: go back and adjust wording so that the use of the term "point" is unambiguous in all contexts. Is a point a \((t,v)\) time-to-value point, a \((\mu,t)\) $\mu$-to-\(t\) point, or a \((\mu,v)\) $\mu$-to-\(v\) point?}

As an alternative, we encode the two parts of the input data readings separately: times are encoded by the \textit{Time Piecewise Polynomial} and values are encoded by the \textit{Value Piecewise Polynomial}. For both piecewise polynomials, let the independent variable $\mu$ be the zero-based index of each successive data sample, that is, the first data sample corresponds to $\mu = 0$, the second data sample corresponds to $\mu = 1$, and so on.



\section{The Context}


\section{The Data}
Our system compresses a sequence of time-sequence data points on-the-fly, by processing each one as it arrives. The first coordinate of a data point is its time value, the second coordinate of a data point is its sample value.

A data point's sample value is a scalar; it is an individual measurement of some real-world quantity reported by a data source (sensor, gauge, etc.).

A data point's time value is the timestamp associated with the sample, which is generally the time at which the sample was collected. The system supports any unit of time (miliseconds, seconds, minutes, etc.), as well as fixed or variable intervals between readings. The system does \textit{not} require an initial time value of zero. Time values are expected to be monotonically increasing, which means that time must not go backwards.

We refer to a raw, uncompressed data point as an \textit{actual} data point.
Let \(p_{actual}\) be an arbitrary actual data point handled by the system, let \(t_{p_{actual}}\) be its time value, and let \(s_{p_{actual}}\) be its sample value:

\[ p_{actual} = (t_{p_{actual}}, s_{p_{actual}}) \]

Actual data point values are exactly equal to the values that were originally reported from the data source.

A zero-based index is designated for each data point when it is received by the system. The index for the first of the sequence of data points is \(\mu = 0\), the index for the second point in the sequence is \(\mu = 1\), and so on.

Let \(n\) be the total count of data points received by the system so far. Then, the index of the most recent point is \(\mu = n-1\). Each index is permanently associated only with its designated data point. All indicies are unique.

Let \(\mu_p\) be the index of data point \(p_{actual}\).

Let \(p_{decoded}\) be the result of encoding, then decoding \(p_{actual}\).\footnote{\(p_{actual}\) and \(p_{decoded}\) may be equal, but in general they will not be.}
Let \(t_{p_{actual}}\) be the time value of \(p_{decoded}\), and let \(s_{p_{actual}}\) be the sample value of \(p_{decoded}\):

\[ p_{decoded} = (t_{p_{decoded}}, s_{p_{decoded}}) \]

Since \(p_{actual}\) and \(p_{decoded}\) both represent the same original data point, \(p_{actual}\) and \(p_{decoded}\) both correspond to the same data point index \(\mu_p\).

\section{The Decoding}
Before showing the encoding process, it is most intuitive to demonstrate how the decoding process uses the encoded data. In \textbf{section X} we will show how input data points are encoded.

If \(p_{actual}\) is the data point before encoding and \(p_{decoded}\) is the data point after decoding, what is the representation of a data point \texit{while} it is encoded? The data point's index \(\mu_p\) is used with the Time Piecewise Polynomial (TPP) and the Sample Piecewise Polynomial (SPP) to generate  \(t_{p_{decoded}}\) and \(s_{p_{decoded}}\), respectively.
Thus, an encoded data point is represented by its index \(\mu_p\) and the corresponding segments of the TPP and the SPP.

\subsection{The Time Piecewise Polynomial (TPP)}
The Time Piecewise Polynomial (TPP) is the piecewise curve used to encode the time values of all data points. The independent variable of the TPP is the index \(\mu\) of the data point. The dependent variable of the TPP is the time \(t\) of the data point.

Let the TPP curve be defined as a function of $\mu$:
\begin{equation*} % instead of equation so we don't get equation numbering...this one is too stronk for that
    \label{eq:T(\mu)}
T(\mu) = \begin{cases}
T_0(\mu) = \tau_{0_{0}}\mu^{k_T} + \tau_{0_{1}}\mu^{k_T-1} + \ldots \tau_{0_{k_T-1}}\mu + \tau_{0_{k_T}} & \text{for } \mu_{\tau 0} \leq \mu < \mu_{\tau 1}, \\
T_1(\mu) = \tau_{1_{0}}\mu^{k_T} + \tau_{1_{1}}\mu^{k_T-1} + \ldots + \tau_{1_{k_T-1}}\mu + \tau_{1_{k_T}} & \text{for } \mu_{\tau 1} \leq \mu < \mu_{\tau 2}, \\
\vdots & \\
T_{n_T-1}(\mu) = \tau_{{n_T-1}_{0}}\mu^{k_T} + \tau_{{n_T-1}_{1}}\mu^{k_T-1} + \ldots + \tau_{{n_T-1}_{k_T-1}}\mu + \tau_{{n_T-1}_{k_T}} & \text{for } \mu_{\tau (n_T-1)} \leq \mu < \mu_{\tau A},

\end{cases}
\end{equation*}

Where:
\begin{itemize}
    \item \(k_T\) is the degree of every fitted curve in the TPP
    \item \(n_T\) is the current count of fitted curve segments in the TPP
    \item For the elements in the sequence $\{\mu_{\tau j} \mid j = 0, 1, \ldots, n_T-1\}$, $\mu_{\tau j}$ is the index of the first data point whose time value is encoded in (and decoded by) the segment polynomial $T_j(\mu)$. The elements of the sequence are generated during the encoding process. \textbf{(See section X.)}
    \item The coefficients \(\tau_{i_{0}}, \tau_{i_{1}}, \ldots \tau_{{i}_{k_T-1}}, \tau_{{i}_{k_T}}\) of segment polynomial $T_i(\mu)$ are generated during the encoding process.
    \item \(A\) is the index of the last data point encoded in (and decoded by) the segment polynomial $T_{n_T-1}(\mu)$. It is generated during the encoding process.
\end{itemize}

\subsection{The Sample Piecewise Polynomial (SPP)}
The Sample Piecewise Polynomial (SPP) is the piecewise curve used to encode the sample values of all data points. The independent variable of the SPP is the index \(\mu\) of the data point. The dependent variable of the SPP is the sample \(s\) of the data point.

Let the SPP curve be defined as a function of $\mu$:
\begin{equation*} % instead of equation so we don't get equation numbering...this one is too stronk for that
    \label{eq:S(\mu)}
S(\mu) = \begin{cases}
S_0(\mu) = \sigma_{0_{0}}\mu^{k_S} + \sigma_{0_{1}}\mu^{k_S-1} + \ldots \sigma_{0_{k_S-1}}\mu + \sigma_{0_{k_S}} & \text{for } \mu_{\sigma 0} \leq \mu < \mu_{\sigma 1}, \\
S_1(\mu) = \sigma_{1_{0}}\mu^{k_S} + \sigma_{1_{1}}\mu^{k_S-1} + \ldots + \sigma_{1_{k_S-1}}\mu + \sigma_{1_{k_S}} & \text{for } \mu_{\sigma 1} \leq \mu < \mu_{\sigma 2}, \\
\vdots & \\
S_{n_S-1}(\mu) = \sigma_{{n_S-1}_{0}}\mu^{k_S} + \sigma_{{n_S-1}_{1}}\mu^{k_S-1} + \ldots + \sigma_{{n_S-1}_{k_S-1}}\mu + \sigma_{{n_S-1}_{k_S}} & \text{for } \mu_{\sigma (n_S-1)} \leq \mu < \mu_{\sigma B},
\end{cases}
\end{equation*}

Where:
\begin{itemize}
    \item \(k_S\) is the degree of every fitted curve in the SPP
    \item \(n_S\) is the current count of fitted curve segments in the SPP
    \item For the elements in the sequence $\{\mu_{\sigma j} \mid j = 0, 1, \ldots, n_S-1\}$, $\mu_{\sigma j}$ is the index of the first data point whose sample value is encoded in (and decoded by) the segment polynomial $S_j(\mu)$. The elements of the sequence are generated during the encoding process. \textbf{(See section X.)}
    \item The coefficients \(\sigma_{i_{0}}, \sigma_{i_{1}}, \ldots \sigma_{{i}_{k_S-1}}, \sigma_{{i}_{k_S}}\) of segment polynomial $S_i(\mu)$ are generated during the encoding process.
    \item \(B\) is the index of the last data point encoded in (and decoded by) the segment polynomial $S_{n_S-1}(\mu)$. It is generated during the encoding process.
\end{itemize}



\subsection{Decoded Data Point Representation}

\subsection{If and When Actual Data Points can be Discarded}
\label{subsec:when_can_discard_point_coords}
\textbf{(This section was written in a legacy context...back when the plan was to use the intermediate matrices approach.)}
The y value (if not also the t value) of a data point can be discarded as soon as the intermediate matrices used to make the segment’s curve fit have been constructed. But when exactly is that? The current open segment must accumulate at least k+1 data points before we can attempt the initial k-degree polynomial fit for that segment. So, in general, the actual point coordinates will be retained until the current open segment has at least k+1 data points. The primary exception is for when the data stream ends while the open segment has k or fewer points. In this case, no k-degree polynomial fit can be attempted on this segment. (See also: \ref{subsec:close_the_final_segment})



\section{The Compression}


\section{Lossiness}


\section{Properties of the System}
Operates on data points, 1D time-series data: (time, atmospheric pressure).
Compression is lossy.
Our system is based on best-fit curves interpolation: the method of least squares.
Our system is not "axis-agnostic:" when measuring error, we measure the distance from each actual data value to its fitted curve parallel with the vertical axis.

\section{Compression Ratio}
During an actual time-series streaming data scenario, there would be no complete, uncompressed, "before" data file on disk to compare our compression results to. For the purpose of this project, we do begin with a complete, uncompressed input data file that we use to simulate the streaming data scenario: we operate on each actual data point from the input sequentially, one at a time, without "peeking" at points that "haven't arrived yet." Our compression ratio comparisons are based on how large the complete, uncompressed data file would be on disk \textit{if} it were stored, which it would not be in practice.


\section{Demonstrations}
(since our system is lossy, we must show “before and after” photos)...current concept is to show, for multiple values of k and epsilon, a "before" scatterplot showing the raw, uncompressed (t,s) data points and then an "after" scatterplot showing the decoded (t,s) data points. Is this also where we would show the (mu, t) and (mu, s) encoded representations of the data?

\section{Limitations}

\subsection{Exceptional Cases}

\subsection{Determinism of Results}
Recall that a closed segment is a segment whose polynomial fit has been finalized, that is, the segment will not accept any additional data points. If you use a t value corresponding to a closed segment for querying its y value, the result will be deterministic: it will evaluate to the same y value every time. Of course, it is not expected to be exactly equal to the actual y value from the original point.

Recall that the current open segment is the segment whose polynomial is *not necessarily* finalized, it is the segment which *may* receive more points before it is ultimately closed. If you use a t value corresponding to the current open segment for querying its y value, the result will *not* be deterministic: it may or may not exactly match the y value from previous or subsequent queries of that same t.
If the curve for the open segment is re-fit, as long as the resulting residual error is within epsilon, that re-fit will become the new fit for the current open segment. It is possible for the curve of the open segment to be re-fit every time a new data point is added. This could happen *many* times before the segment is closed.

Hopefully this is not an issue for the client, as all of the queried y values will be estimates anyway and the re-fit had to meet the residual error bound requirement in order to be accepted in the first place. Still, the non-deterministic nature of y lookups in the open segment is something to be aware of while using this system. Any code that would rely on deterministic y value lookups within the open segment would be incorrect.


\section{Further Ideas}


\section{PYTHON DEVELOPMENT TO DO LIST}

\subsection{Size Comparison (for Compression section)}
Initially, compare theoretical data sizes in bytes between the raw data and the encoded data (coefficients for the k polynomial of each segment, start time t of each segment, any other metadata required to decode).

Eventually, actually write the data to file to compare file sizes on disk. Suggest using a simple text file with spaces and newlines as delimiters, rather than doing more elaborate compression on top of our basic scheme. There is enough complexity in our system as it is.
We could mention ideas for further compression in the “Possible Extensions” section.

\subsection{Time-Delay Simulation}
Visually simulate the time-delay of the incoming stream of data. This would be for the purpose of animating the curve fit over time as data points arrive and visually growing the overall piecewise curve for the data set as it accumulates.

Possibly matplotlib animation features? I couldn’t get them to work. Plotly also looked promising, but it seems to require webserver setup, etc.
I will defer to the judgment of whomever works on this.


\subsection{Close the Final Segment}
\label{subsec:close_the_final_segment}
When the data stream ends, formally close the current open segment. Current best idea is to not encode these points at all and simply store the actual t and y values to be appended to the end of the decoded data. (See also \ref{subsec:when_can_discard_point_coords})


\subsection{Data Values with Frequency Greater than One}
Clearly, any given property value may appear any number of times in the data set. (Example.) Further, equal property values generally will not occur at adjacent values of mu, nor will they be associated with any particular value of t.
But what about time values? Thanks to our parametric definition, there is no reason to restrict encoding multiple data points with the same time value. However, the Time Piecewise Polynomial is designed with the monotonically increasing property of time in mind, so for any times that appear in the data set more than once, we will require that they appear at consecutive values of mu. 
(We are leveraging the fact that time is monotonically increasing: time never goes backwards.)


\end{document}
